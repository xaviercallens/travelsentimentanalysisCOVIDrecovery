{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"EntrainementModeleConvolution.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qLjIVB1TP0Ba"},"source":["\n","\n","# Les Import :"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tFjwTIfIZyQA","executionInfo":{"status":"ok","timestamp":1617116957105,"user_tz":-120,"elapsed":10225,"user":{"displayName":"adresse 2","photoUrl":"","userId":"18136064469607769711"}},"outputId":"9e399abe-e30c-4c48-9bef-fa25e09e43de"},"source":["!pip install bert-for-tf2\n","!pip install sentencepiece\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting bert-for-tf2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/a1/acb891630749c56901e770a34d6bac8a509a367dd74a05daf7306952e910/bert-for-tf2-0.14.9.tar.gz (41kB)\n","\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n","\u001b[?25hCollecting py-params>=0.9.6\n","  Downloading https://files.pythonhosted.org/packages/aa/e0/4f663d8abf83c8084b75b995bd2ab3a9512ebc5b97206fde38cef906ab07/py-params-0.10.2.tar.gz\n","Collecting params-flow>=0.8.0\n","  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n","Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n","  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-cp37-none-any.whl size=30535 sha256=13b43d82ce1764cc3da1b54e0ac2d6034703081b60c91c6f1c10c317ec0531a1\n","  Stored in directory: /root/.cache/pip/wheels/a1/04/ee/347bd9f5b821b637c76411d280271a857aece00358896a230f\n","  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for py-params: filename=py_params-0.10.2-cp37-none-any.whl size=7912 sha256=ead8740852569f3dfff6b5c770d77a396f811fc457014b12319262955264027c\n","  Stored in directory: /root/.cache/pip/wheels/d0/4a/70/ff12450229ff1955abf01f365051d4faae1c20aef53ab4cf09\n","  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for params-flow: filename=params_flow-0.8.2-cp37-none-any.whl size=19472 sha256=f3fde3db8eb00863ec4a4930847cc4b74db12023925585cefdae712b889a7387\n","  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n","Successfully built bert-for-tf2 py-params params-flow\n","Installing collected packages: py-params, params-flow, bert-for-tf2\n","Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 17.3MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.95\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tv-iv2ub09a8","executionInfo":{"status":"ok","timestamp":1617116962968,"user_tz":-120,"elapsed":3578,"user":{"displayName":"adresse 2","photoUrl":"","userId":"18136064469607769711"}},"outputId":"1d793567-f0de-4b9a-856a-abab69325076"},"source":["import tensorflow as tf\n","\n","import tensorflow_hub as hub\n","\n","from tensorflow.keras import layers\n","import bert\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import matplotlib.pyplot as plt\n","\n","import nltk\n","nltk.download('wordnet')\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","from bs4 import BeautifulSoup #to extract words from HTML documents\n","\n","import string\n","import matplotlib.pyplot as plt\n","from keras.utils import to_categorical\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.preprocessing.text import Tokenizer\n","from keras.layers import *\n","from keras.callbacks import EarlyStopping\n","from keras.losses import categorical_crossentropy\n","from keras.optimizers import Adam,Adagrad,Adadelta,Adamax\n","from keras.models import Sequential\n","\n","from random import shuffle\n","def shuffle_list(*ls):\n","  l =list(zip(*ls))\n","  shuffle(l)\n","  return zip(*l)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HjYDs8Zv1Ar0"},"source":["# DL des datas :"]},{"cell_type":"markdown","metadata":{"id":"mX-xjdMlJ5G2"},"source":["Cette section sert a download les datas. Il faut donc la modifier selon la facon dont on utilise ce fichier."]},{"cell_type":"code","metadata":{"id":"qGxwsBjMW3mG"},"source":["!pip install PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IaHRTVTB1lQJ"},"source":["\"train.zip\" :"]},{"cell_type":"code","metadata":{"id":"aMCsulQT1jZv"},"source":["downloaded = drive.CreateFile({'id':\"1l6OX6DbY00CE392f-cYZtOZ0lkM\"})   # replace the id with id of file you want to access\n","downloaded.GetContentFile('train.tsv.zip')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_lvWLva51tvW"},"source":["\"Tweets-Flight.csv\" :"]},{"cell_type":"code","metadata":{"id":"zyvE-5ra1aWJ"},"source":["downloaded = drive.CreateFile({'id':\"1V96pme-eFHBOerhM79tNhRtwoh2VNkDD\"})   # replace the id with id of file you want to access\n","downloaded.GetContentFile('Tweets-Flight.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EzSoxfYb2GFC"},"source":["On va load le fichier des datas. Exexuter la cellule qui correspond au bon fichier. NE PAS executer les 2 cellules."]},{"cell_type":"markdown","metadata":{"id":"pumgfFQWiuuE"},"source":["# **MovieReviews :**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-8Olh9uJ6j1h","executionInfo":{"status":"ok","timestamp":1616583193867,"user_tz":-60,"elapsed":1834,"user":{"displayName":"oldragon ar","photoUrl":"","userId":"11831130061906073290"}},"outputId":"4a2ee579-8d74-4e1b-c130-2b891c8d8fe3"},"source":["movie_reviews = pd.read_csv(\"train.tsv.zip\", sep=\"\\t\")\n","rev0=movie_reviews['Phrase']\n","sentiment0=movie_reviews.Sentiment.values\n","#y=to_categorical(movie_reviews.Sentiment.values)\n","print(\"Nombre total de Reviews :\"+ str(len(sentiment0)))\n","\n","rev,sentiment=shuffle_list(rev0,sentiment0)\n","\n","# On re-equilible la base de données : \n","\n","n=120000\n","\n","k1=0\n","k2=0\n","k3=0\n","y0=[]\n","reviews0=[]\n","for i in range(len(sentiment)):\n","  el=sentiment[i]\n","  if el<=1 and k1<n/3 : \n","    k1+=1\n","    y0.append([1,0,0])\n","    reviews0.append(rev[i])\n","  if el==2 and k2<n/3:\n","    k2+=1\n","    y0.append([0,1,0])\n","    reviews0.append(rev[i])\n","  if el>=3 and k3<n/3 :\n","    k3+=1\n","    y0.append([0,0,1])\n","    reviews0.append(rev[i])\n","\n","\n","y=np.array(y0,dtype='float32')\n","reviews=np.array(reviews0)\n","print(\"Nombre total de Reviews apres equilibrage :\"+ str(len(y)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Nombre total de Reviews :156060\n","Nombre total de Reviews apres equilibrage :114345\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YhhouNT8irs2"},"source":["# **OU**"]},{"cell_type":"markdown","metadata":{"id":"prp3KJtfi1R6"},"source":["# **Tweets-flight :**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xCNKsu0VU29i","executionInfo":{"status":"ok","timestamp":1617116988936,"user_tz":-120,"elapsed":605,"user":{"displayName":"adresse 2","photoUrl":"","userId":"18136064469607769711"}},"outputId":"1b2df0e3-22e4-4705-f9f5-411dce42058e"},"source":["tweet_reviews = pd.read_csv('Tweets-Flight.csv')\n","rev=np.array(tweet_reviews['text'])\n","sentiment=tweet_reviews['airline_sentiment']\n","y0=[]\n","reviews0=[]\n","\n","# On re-equilible la base de données : on doit remove 5500 'neutral'\n","n=9178-5500\n","k=0\n","\n","for i in range(len(sentiment)):\n","  el=sentiment[i]\n","  if el=='negative' and k<n:\n","    k+=1 \n","    y0.append([1,0,0])\n","    reviews0.append(rev[i])\n","\n","  if el=='neutral': \n","    y0.append([0,1,0])\n","    reviews0.append(rev[i])\n","\n","  if el=='positive': \n","    y0.append([0,0,1])\n","    reviews0.append(rev[i])\n","\n","y=np.array(y0,dtype='float32')\n","reviews=np.array(reviews0)\n","\n","print(\"Nombre total de Tweets :\"+ str(len(reviews)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Nombre total de Tweets :9140\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Kv4seKJ4jEqg"},"source":["Fin du load\n","\n","Visualisation des datas :"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QQzRZ2J2c9js","executionInfo":{"status":"ok","timestamp":1617110768838,"user_tz":-120,"elapsed":650,"user":{"displayName":"Clément Bureau","photoUrl":"","userId":"08967041074233290673"}},"outputId":"0b765037-d58c-49a6-ffe7-fc5926116fbf"},"source":["print(reviews)\n","print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['@VirginAmerica What @dhepburn said.'\n"," \"@VirginAmerica plus you've added commercials to the experience... tacky.\"\n"," \"@VirginAmerica I didn't today... Must mean I need to take another trip!\"\n"," ... '@AmericanAir thank you we got on a different flight to Chicago.'\n"," '@AmericanAir Please bring American Airlines to #BlackBerry10'\n"," '@AmericanAir we have 8 ppl so we need 2 know how many seats are on the next flight. Plz put us on standby for 4 people on the next flight?']\n","[[0. 1. 0.]\n"," [0. 0. 1.]\n"," [0. 1. 0.]\n"," ...\n"," [0. 0. 1.]\n"," [0. 1. 0.]\n"," [0. 1. 0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mNOnTWTw1OSY"},"source":["# **Shuffle Des données :**\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"kIUvQP41ym6w"},"source":["y,reviews= shuffle_list(y,reviews)\n","y=np.array(y)\n","reviews=np.array(reviews)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X81sKZ4-uay1"},"source":["# Remove Stop-Words :"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sEDXtQPtugcD","executionInfo":{"status":"ok","timestamp":1617116993941,"user_tz":-120,"elapsed":1215,"user":{"displayName":"adresse 2","photoUrl":"","userId":"18136064469607769711"}},"outputId":"980bcf4f-8d3a-47ef-b338-582d7f8c3e63"},"source":["import nltk\n","import re\n","# Uncomment to download \"stopwords\"\n","nltk.download(\"stopwords\")\n","from nltk.corpus import stopwords\n","\n","def text_preprocessing(s):\n","    \"\"\"\n","    - Lowercase the sentence\n","    - Change \"'t\" to \"not\"\n","    - Remove \"@name\"\n","    - Isolate and remove punctuations except \"?\"\n","    - Remove other special characters\n","    - Remove stop words except \"not\" and \"can\"\n","    - Remove trailing whitespace\n","    \"\"\"\n","    s = s.lower()\n","    # Change 't to 'not'\n","    s = re.sub(r\"\\'t\", \" not\", s)\n","    # Remove @name\n","    s = re.sub(r'(@.*?)[\\s]', ' ', s)\n","    # Isolate and remove punctuations except '?'\n","    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n","    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n","    # Remove some special characters\n","    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n","    # Remove stopwords except 'not' and 'can'\n","    s = \" \".join([word for word in s.split()\n","                  if word not in stopwords.words('english')\n","                  or word in ['not', 'can']])\n","    # Remove trailing whitespace\n","    s = re.sub(r'\\s+', ' ', s).strip()\n","    \n","    return s"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7lL0ArAlui1K"},"source":["reviews=np.array([text_preprocessing(s)for s in reviews])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mbJGOgiWO29z"},"source":["# **Tokenizer :**"]},{"cell_type":"code","metadata":{"id":"QSMfTaxmU8vh"},"source":["BertTokenizer = bert.bert_tokenization.FullTokenizer\n","bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n","                            trainable=False)\n","vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n","to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n","tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B6aDPhAeVDwd"},"source":["def tokenize_reviews(text_reviews):\n","    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8snpuP19VDpd"},"source":["tokenized_reviews = [tokenize_reviews(review) for review in reviews]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PpmKPcbdPKzq"},"source":["# **Padding :**"]},{"cell_type":"code","metadata":{"id":"E5ueHoE2irIy"},"source":["def PaddingX(M,pad=-1):\n","  if pad==-1: \n","    pad = len(max(M, key=len))\n","  return np.array([i + [0]*(pad-len(i)) for i in M]),pad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PUX8Atheja7A","executionInfo":{"status":"ok","timestamp":1617117027960,"user_tz":-120,"elapsed":26913,"user":{"displayName":"adresse 2","photoUrl":"","userId":"18136064469607769711"}},"outputId":"67e25fc6-8de5-470d-b03d-033a9890b7ce"},"source":["#padded_reviews,padd=PaddingX(tokenized_reviews,padd)\n","padded_reviews,padd=PaddingX(tokenized_reviews)\n","\n","print(padd)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["40\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZrI9tDSFCgbv"},"source":["# **Separation train/validation :**"]},{"cell_type":"code","metadata":{"id":"B142OMhRCgED"},"source":["# Create X and Y\n","y_target=y\n","num_classes = len(y_target[0])\n","\n","X_train,X_val,y_train,y_val = train_test_split(padded_reviews, y_target, test_size=0.2, stratify=y_target)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QfukRGHuA5mI"},"source":["# **Définition du réseau Convolution :**"]},{"cell_type":"code","metadata":{"id":"BOGmHApnvyoG"},"source":["def Model_CV_PL_FL(with_early_stop=False):\n","  D = 50 #embedding dimensionality\n","  dropout_rate = 0.5\n","\n","  # Define early stopping that will be used as callback\n","  early_stopping = EarlyStopping(min_delta=0.01, mode='max', monitor='val_accuracy', patience=3) # min_delta par defaut = 0.01\n","\n","\n","  if with_early_stop : callback=[]\n","  else :callback = [early_stopping]\n","\n","\n","  sequence_length=X_train.shape[1]\n","\n","\n","  # add layers to model\n","  model = Sequential()\n","  model.add(Embedding(len(tokenizer.vocab), D))\n","  model.add(Reshape((sequence_length,D,1)))\n","  model.add(Conv2D(40, (3, D), activation='relu' ))\n","  model.add(GlobalMaxPooling2D())\n","  model.add(Dense(50, activation='relu'))\n","  model.add(Dropout(dropout_rate))\n","  model.add(Dense(num_classes,activation='softmax'))\n","\n","  # use a relatively low learning rate\n","  model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])\n","\n","  model.summary()\n","\n","  r = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=256, callbacks=callback)\n","  #plt.plot(r.history['loss'][5:])\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QwX7lB3ljgsG"},"source":["# **Apprentissage des Modeles :**"]},{"cell_type":"code","metadata":{"id":"avux_T0hhrHc"},"source":["model_trained=Model_CV_PL_FL(with_early_stop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZQEz1XolkXer"},"source":["# **Détails des resultats :**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lWFwNBGGkXJt","executionInfo":{"status":"ok","timestamp":1617112311667,"user_tz":-120,"elapsed":2603,"user":{"displayName":"Clément Bureau","photoUrl":"","userId":"08967041074233290673"}},"outputId":"edc3ca06-1173-4979-884b-f09dd39ac2b5"},"source":["Y_sortie_proba=model_trained.predict(X_val)\n","Y_sortie=Y_sortie_proba.argmax(1)\n","Y_val=y_val.argmax(1)\n","\n","Matrice=np.zeros((3,3))\n","for i in range(len(Y_sortie)):\n","  Matrice[Y_sortie[i]][Y_val[i]]+=1\n","\n","reussite=[0 for i in range(num_classes)]\n","for i in range(len(y_val)):\n","  reussite[Y_val[i]]+=(Y_sortie[i]==Y_val[i])\n","\n","print(\"Nombre de reussites : \" + str(sum(reussite))+\" / \"+str(len(Y_val)) )\n","taux_reussite=[reussite[i]/sum(Y_val==i) for i in range(num_classes)]\n","print(\"Pourcentage de reussites par sentiment : \" + str(taux_reussite))\n","print(\"Matrice des resultats : \")\n","print(Matrice)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Nombre de reussites : 1348 / 1828\n","Pourcentage de reussites par sentiment : [0.8394557823129252, 0.6338709677419355, 0.7145877378435518]\n","Matrice des resultats : \n","[[617. 153.  71.]\n"," [ 87. 393.  64.]\n"," [ 31.  74. 338.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CQniYpyQzuMN"},"source":["# **Test Manuel de Phrases :**"]},{"cell_type":"code","metadata":{"id":"icXRhqTWEqZn"},"source":["# Donne les probas données par le modele 'model' sur la phrase 'sentence'\n","# Exemple : proba = evaluate_proba(model,'The flight is good')\n","\n","def evaluate_proba(model,sentence):\n","  sentence_preprocessed = text_preprocessing(sentence)\n","  #print(sentence_preprocessed)\n","  sentence_tokenized = tokenize_reviews(sentence_preprocessed)\n","  sentence_padded,p =PaddingX([sentence_tokenized],padd)\n","  return model.predict(sentence_padded)\n","\n","\n","\n","# Donne le sentiment donné par le modele 'model' sur la phrase 'sentence'   (ATTENTION A COMMENT SONT NUMEROTES LES SENTIMENTS)\n","# Exemple : sentiment = evaluate_sentiment(model,'The flight is good')\n","def evaluate_sentiment(model,sentence):\n","  return np.argmax(evaluate_proba(model,sentence))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Zv_sptZLz2L"},"source":["Ici vous pouvez tester le modèle sur des exemples :"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RVqGcXexGQDq","executionInfo":{"status":"ok","timestamp":1614506372451,"user_tz":-60,"elapsed":410,"user":{"displayName":"adresse 2","photoUrl":"","userId":"18136064469607769711"}},"outputId":"010eb3a5-a9c5-4add-9470-dc023c86f895"},"source":["# Exemple :  proba = evaluate_proba(model,'The flight is good')\n","#            sentiment = evaluate_sentiment(model,'The flight is good')\n","\n","#sentence = \"this flight is good\"\n","sentence = \"this flight is not the best\"\n","#sentence = \"I love this movie\"\n","#sentence = \"I don't love this movie\"\n","#sentence = \"I wish this movie was better\"\n","\n","proba = evaluate_proba(model_trained,sentence)\n","print(\"Les probas sont : \"+str(proba))\n","sentiment= evaluate_sentiment(model_trained,sentence)\n","print(\"Le sentiment est : \"+str(sentiment))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Les probas sont : [[0.0759319  0.7354456  0.18862253]]\n","Le sentiment est : 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-uMQ0MYFOuNu"},"source":["# Sauvegarde du Modele"]},{"cell_type":"markdown","metadata":{"id":"8ZL0LP07biXX"},"source":["Sauvegarde le modele sur le drive du proprietaire :"]},{"cell_type":"code","metadata":{"id":"BK8viCjaPdXh"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","!ls '/content/gdrive/My Drive'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n0KRXlL1RmSy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616586169882,"user_tz":-60,"elapsed":7651,"user":{"displayName":"oldragon ar","photoUrl":"","userId":"11831130061906073290"}},"outputId":"58a129e4-0cd0-4bb8-8d08-8dd11da95ca3"},"source":["model_trained.save('/content/gdrive/My Drive/model_Tweets_Flights')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/gdrive/My Drive/model_Tweets_Flights_pad70/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/gdrive/My Drive/model_Tweets_Flights_pad70/assets\n"],"name":"stderr"}]}]}